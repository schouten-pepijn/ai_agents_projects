from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from graphstate import GraphState
from helpers import _sources_table
from config import graph
import uuid


def run_research_assistant(question: str, rounds: str, max_results_per_query: int):
    """
    Runs the research assistant workflow for a given question.
    Args:
        question (str): The research question to be answered.
        rounds (str): The number of search/answering rounds to perform.
        max_results_per_query (int): Maximum number of results to retrieve per query.
    Returns:
        tuple:
            - answer (str): The final answer generated by the assistant.
            - sources (list): A list of sources or references used to generate the answer.
            - actions (str): A string describing the sequence of actions taken during the workflow.
    Notes:
        - If the question is empty or invalid, returns an error message and empty results.
        - The function initializes the assistant's state and invokes the research workflow graph.
    """
    if not question or not question.strip():
        return "Please enter a valid question.", [], ""
    
    init_state: GraphState = {
        "messages": [HumanMessage(content=question.strip())],
        "queries": [],
        "hits": [],
        "round_idx": 0,
        "max_rounds": max(1, int(rounds)),
        "max_results_per_query": max(2, int(max_results_per_query)),
        "answer": "",
        "actions": [],
    }
    
    final_state = graph.invoke(
        init_state,
        config=RunnableConfig(
            run_name="research_assistant_run",
            configurable={"thread_id": str(uuid.uuid4())}
        )
    )

    answer = final_state["answer"]
    sources = _sources_table(final_state["hits"])
    actions = " -> ".join(final_state["actions"])

    return answer, sources, actions